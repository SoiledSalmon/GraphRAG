"""
This module serves as an isolated client for interacting with the Google Gemini API.
It handles prompt transmission and response retrieval while managing errors and
configuration internally, ensuring the rest of the backend remains decoupled from
specific LLM implementation details.
"""

import os
import logging

import google.generativeai as genai

logger = logging.getLogger(__name__)

# Configuration
_MODELS_TO_TRY = ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash", "gemini-2.0-flash-001"]
_FALLBACK_MESSAGE = "The system is temporarily unavailable."

# Module-level flag to track SDK configuration
_sdk_configured = False


def _configure_sdk() -> None:
    """Configure the Gemini SDK with the API key (once per process)."""
    global _sdk_configured
    if _sdk_configured:
        return

    api_key = os.environ.get("GEMINI_API_KEY")

    if not api_key:
        logger.error("GEMINI_API_KEY is not set in environment")
        raise RuntimeError("GEMINI_API_KEY is not set. Please add it to your .env file.")

    if "your" in api_key.lower() and "key" in api_key.lower():
        logger.error("GEMINI_API_KEY contains placeholder value: %s", api_key)
        raise RuntimeError(
            f"GEMINI_API_KEY contains a placeholder value '{api_key}'. "
            "Please replace it with your actual API key in .env"
        )

    genai.configure(api_key=api_key)
    _sdk_configured = True
    logger.debug("Gemini SDK configured successfully")


def generate_response(prompt: str) -> str:
    """
    Sends a prompt to the Gemini API and returns the generated text.
    Retries with fallback models if a quota exhaustion (429) error occurs.

    Args:
        prompt (str): The fully constructed natural language prompt.

    Returns:
        str: The text generated by the LLM, or a fallback message on failure.
    """
    try:
        _configure_sdk()

        for model_name in _MODELS_TO_TRY:
            try:
                logger.debug(f"Attempting to generate response with model: {model_name}")
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)

                if not response.text:
                    logger.debug(f"Gemini response from {model_name} has no text content")
                    continue # Try next model or return fallback? 
                           # Actually emptiness might not be a 429, but let's assume if one fails we try others?
                           # No, only 429 is worth retrying for quota. Empty text is a model behavior issue.
                           # For now, let's return if we get a response.
                    return _FALLBACK_MESSAGE

                return response.text

            except Exception as e:
                error_str = str(e)
                # Check for 429 Quota Exceeded (ResourceExhausted)
                if "429" in error_str or "quota" in error_str.lower() or "resource exhausted" in error_str.lower():
                    logger.warning(f"Quota exceeded for {model_name}. Retrying with next model...")
                    continue
                else:
                    # Reraise or log other errors? 
                    # If it's not a quota error, it might be a prompt safety error or network error.
                    # We should probably try other models even for safety errors sometimes, but strictly for 429 here.
                    logger.error(f"Gemini API request failed for {model_name}: {e}")
                    # If it's a critical error (not 429), maybe we shouldn't retry?
                    # But to be robust, let's continue to the next model just in case it's model-specific.
                    continue

        # If all models fail
        logger.error("All Gemini models failed.")
        return _FALLBACK_MESSAGE

    except RuntimeError:
        # Re-raise configuration errors (missing/invalid API key)
        raise
    except Exception as e:
        logger.error("Gemini API request failed: %s", e)
        return _FALLBACK_MESSAGE
